---
title: 第一次的机器学习：决策树与随机森林
date: 2017-08-15
category: 知识课堂
tags: 
- 机器学习
---



#概述

* 决策树是一种符合人类直觉的方式。
* 决策树是一种常见的**监督学习**方法，是**随机森林**的基础。
* 决策树分为两种：**分类树**和**回归树**。顾名思义，分类树的目的是解决监督学习中的分类问题，而回归树是为了解决回归问题，其差别也就是输出值是离散的或是连续的差别。同时也有人提出了**分类和回归树（CART）**，来处理分类树和回归树异同。
* 从算法的角度来说，决策树的训练方法是**贪心**的，建立树的过程使用**递归**的方法。



# 如何种一棵决策树？

## 直觉

不是有了机器学习才有决策树，而是决策树为机器学习所使用。决策树本身是十分符合人类直觉的。例如，今天的晚餐，如果三个人并且家里没有菜就去外面搓一顿，如果三个人并且家里有菜就一起在家吃，如果只有一个人或两个人就点外卖，这就是一棵简单的决策树。

![一颗简单的决策树](/Users/Linda/Downloads/Untitled Diagram.png)

在这个问题中，决定『今晚吃什么』是一个**决策**，而决定人数、家里是否有菜是**子决策**。在一个决策的过程中，我们自然而然地面临了多个子决策，并做出选择，从而形成一棵决策树。

当然，机器学习中的决策树往往不只这么简单——我们遇到的大部分数据都会出现多个特征，而有时候特征的数量会多达几十甚至更多，此时如果要考虑所有的特征，树的复杂度将会级数增长。生活中的例子往往可以简单地用头脑思考哪个特征更好而哪个特征更差，可如果要种下一棵巨大的决策树需要遭遇成千上万的子决策，而它们的数量和质量往往并非人脑可以判定的问题。

## 基本算法

如上文所说，决策树学习的基本算法是通过递归来建立决策树——面对每一个决策，我们将其分为子决策建树，直至递归结束的情形。对于一个训练集、属性集，我们自然而然地将递归结束结点的类型分为三种情形：

1. 当前结点包含的样本全属于同一类别，无需划分
2. 当前属性集为空，或是所有样本在所有属性上的取值相同，无法划分
3. 当前结点包含的样本集合为空，无法划分

![desicion-tree-1](/Users/Linda/Downloads/desicion-tree-1.png)

后两种情形的看似处理方式类似，但是其背后蕴含的想法是不同的。情形2，我们使用当前结点的后验分布（也就是利用已知的数据）来确定当前结点的属性；而情形3，我们将父结点的属性作为当前结点的先验分布（也就是在没有信息的情况下认为该结点与父结点属性相同）。

这种分而治之的递归法会最终一整棵决策树，包含样本全集，这样的决策树能够处理未见样例，有很高的泛化性。

（参考：周志华《机器学习》）

## 最优划分

上文中算法的第九行提到了选出最优划分属性，而这种属性是什么呢？事实上，这种最优划分的属性可以有很多种，因而也出现了很多不同的算法

### ID3 算法

ID3 (Iterative Dichotomiser 3) 是最为简单的生成决策树算法之一，而它使用的最优划分衡量标准是**熵（Entropy）**，或者说是**信息增益（Information Gain）**。

#### 熵

熵可以用如下公式来计算：
$$
H(S) = \sum_{x\in X}p(x)\log_2(1/p(x)) = \sum_{x\in X}-p(x)\log_2(p(x))
$$
其中，S 是当前数据集，当前数据集由当前结点决定，因此每一次递归过程中的数据集是不相同的；X 是属性集；而 p(x) 是属性 x 的样本在全部样本中所占的比例。如果 H(S) = 0，说明该集合 S 被完美分类。

沿用上次的例子，并将问题简化为一个二元分类问题：

| 城市   | 最高温度 | 最低温度 | 相对湿度 | 某时刻风速    | 天气情况 |
| ---- | ---- | ---- | ---- | -------- | ---- |
| A市   | 36℃  | 28℃  | 58%  | 16.7km/h | 好天   |
| B市   | 28℃  | 17℃  | 86%  | /        | 坏天   |
| C市   | 34℃  | 29℃  | 39%  | 20.4km/h | 好天   |

那么该数据集的熵根据天气情况分类（好天、坏天）可以如下计算：
$$
H(S) = - \frac{2}{3} \times \log_2(\frac{2}{3}) - \frac{1}{3} \times \log_2(\frac{1}{3}) \approx 0.918
$$
需要注意的是，如果三个城市都是好天的话，熵则会变为0：
$$
H(S) = - 1 \times \log_2(1) = 0
$$

#### 信息增益

信息增益则是在分类前后熵的变化的对比，其基础就是熵的计算。利用数学式来表达：
$$
IG(A, S) = H(S) - \sum_{t\in T}p(t)H(t)
$$
其中，A 是某个特征，而 T 是通过 A 特征所划分出的类别。p 和 H 的定义与上文保持一致。

例如，我们根据条件**相对湿度 > 50%**分类，那么数据集会被我们分为两类：A市和B市为一类、C市为第二类。第一步我们需要分别计算他们的熵：
$$
H(1) = - \frac{1}{2} \times \log_2(\frac{1}{2}) - \frac{1}{2} \times \log_2(\frac{1}{2}) \approx 1\\
H(2) = - 1 \times \log_1 \approx 0
$$
其中，H(1) 是 A、B 两市的熵，而 H(2) 是 C 市的熵。随后信息增益就会被表示为：
$$
IG(湿度>50\%, S) = 0.918 - (\frac{2}{3}\times 1 + \frac{1}{3}\times0) \approx 0.2513
$$
再例如，我们根据条件**相对湿度 > 60%**分类，那么数据集同样会被我们分为两类：A市和C市为一类、B市为第二类。我们再次分别计算他们的熵：
$$
H(1) = - 1 \times \log_1 \approx 0\\
H(2) = - 1 \times \log_1 \approx 0
$$
对于当前数据集来说，这是一个完美的分类，因此熵为0。我们紧接着计算信息增益：
$$
IG(湿度>60\%,  S) = 0.918 - (\frac{2}{3}\times 0 + \frac{1}{3}\times0) = 0.918
$$
由上述例子我们可以发现，一个好的决策，分类后的熵值变小，因而信息增益会更大。而 ID3 算法就是在每个结点选择信息增益最大的决策。



### C4.5 算法

尽管 ID3 算法看起来无懈可击，但是事实上有一个巨大的 bug：如果我们按照**名称**分类，将上述例子分为 A、B、C 三类，我们会收获一个极好的分类结果——但是这个分类结果没有任何作用，因为没有泛化性，也就是说，来一个新的名为 D市 的城市，我们是无法对其进行预测或是分类的。用更加专业的话来说，这种方法极容易导致过拟合的情况。

#### 增益率

在 C4.5 算法中，最优划分从信息增益变为了增益率（Gain ratio），其计算公式除了考虑到信息增益以外，还考虑了属性 A 可能的数值数量：
$$
GainRatio(A, S) = \frac{Gain(A, S)}{IV(A)}
$$
其中，
$$
IV(A) = -\sum_{t \in T}\frac{\lvert S_t \rvert}{\lvert S \rvert}\times\log_2(\frac{\lvert S_t \rvert}{S})
$$
例如，
$$
IV(湿度 > 60\%) = -\frac{1}{3}\log_2(\frac{1}{3}) -\frac{2}{3}\log_2(\frac{2}{3}) \approx 0.918
$$
而
$$
IV(城市) = -\frac{1}{3}\log_2(\frac{1}{3}) -\frac{1}{3}\log_2(\frac{1}{3})-\frac{1}{3}\log_2(\frac{1}{3})\approx 1.59
$$
由此可见，分为越多类的属性 IV 的值就会越大，因而 GainRatio 就会变小。这也是对 ID3 算法过拟合的情况作出的一种调整。



### CART

CART (Classification and Regression Tree) 是一种既可以分类也可以回归的决策树。在这种决策树中，使用的最优划分是基尼系数（Gini index）来决定划分。

#### 基尼值

基尼值反映了从数据集中随机抽取两个样本，其类别不一致的概率：
$$
Gini(S) = \sum_{x \in X}\sum_{x' \neq x}p(x)p(x')  = 1 - \sum_{x\in X}p(x)^2
$$
基尼值表现了一个数据集的纯度，一个数据集的基尼值越小，数据的纯度越高。

#### 基尼系数

基尼值本身无法确定如何选择划分属性，而基尼系数解决了这个问题：
$$

$$

$$
GiniIndex(A, S) = \sum_{t \in T} \frac{\lvert S_t \rvert}{\lvert S \rvert} Gini(S_t)
$$

在决策树问题中，我们会选择划分后基尼系数最小的属性。

## 剪枝

